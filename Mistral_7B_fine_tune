# -*- coding: utf-8 -*-
"""Copy of Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmUFL5KVcTlWCZr3Tq4zFI3z01Q66RsK
"""

from google.colab import drive
drive.mount('/content/drive')
!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

#!/usr/bin/env python
# coding: utf-8
!pip install transformers datasets peft trl torch bitsandbytes accelerate evaluate google-colab
!pip install fsspec==2024.6.1
!pip install gcsfs==2024.9.0

# Check the formatted output
if __name__ == "__main__":
    input_file = "/content/drive/MyDrive/smart_home_chatbot/smart_home_dialogues_final.json"
    output_dir = "output_data"
    os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist
    train_data, eval_data = reformat_and_split(input_file, output_dir)

# Set up the model and tokenizer

# In[79]:


from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from peft import prepare_model_for_kbit_training
from huggingface_hub import login


token = "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"
login(token=token)

import torch
torch.cuda.empty_cache()

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model_id = "mistralai/Mistral-7B-Instruct-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.padding_side = "right"

from accelerate import init_empty_weights
from accelerate.utils import set_module_tensor_to_device

with init_empty_weights():
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",  # Automatically balance between CPU and GPU
        low_cpu_mem_usage=True,  # Reduces memory during loading
        token=token
    )


model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)


# Step 3: Set up PEFT (Parameter-Efficient Fine-Tuning)

# In[81]:


from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ],
    bias="none",
    lora_dropout=0.1,
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, config)


# Step 4: Set up the training arguments

# In[ ]:


from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="smart_home_chatbot",
    overwrite_output_dir=True,
    logging_dir="./logs",
    logging_steps=50,
    logging_first_step=True,            # Log the first step
    evaluation_strategy="steps",        # Evaluate during training at each `eval_steps`
    eval_steps=500,                      # Evaluation interval
    save_steps=500,                      # Save checkpoint interval
    save_total_limit=1,                 # Only keep the last 2 checkpoints
    disable_tqdm=False,                 # Show the progress bar
    num_train_epochs=3, # replace this, depending on your dataset
    per_device_eval_batch_size=4,       # Batch size for evaluation
    per_device_train_batch_size=4,
    gradient_accumulation_steps=32,
    learning_rate=1e-4,
    optim="adamw_8bit",
    fp16=True,
    report_to=["tensorboard"],
    dataloader_pin_memory=True,
    tf32=True
)


# Step 5: Initialize the trainer and fine-tune the model

# In[ ]:


from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

train_dataset = load_dataset("json", data_files="output_data/train_data.json")["train"]
eval_dataset = load_dataset("json", data_files="output_data/eval_data.json")["train"]


# Initialize the SFTTrainer
trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    max_seq_length=256,                # Pass directly
    dataset_text_field="text",         # Pass directly
    eval_dataset=eval_dataset
)

# Start training
trainer.train()

# Step 6: Merge the adapter and model back together

# In[ ]:


adapter_model = trainer.model
merged_model = adapter_model.merge_and_unload()

trained_tokenizer = trainer.tokenizer

