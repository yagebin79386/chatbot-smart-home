# -*- coding: utf-8 -*-
"""Copy of Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmUFL5KVcTlWCZr3Tq4zFI3z01Q66RsK
"""

from google.colab import drive
drive.mount('/content/drive')
!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

#!/usr/bin/env python
# coding: utf-8
!pip install transformers datasets peft trl torch bitsandbytes accelerate evaluate google-colab
!pip install fsspec==2024.6.1
!pip install gcsfs==2024.9.0

from datasets import Dataset
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

#load the dataset
import json

dataset = None


def flatten_data(input_file):
    with open(input_file, "r") as f:
        data = json.load(f)

    # Flatten the data if it's nested (as seen in your dataset)
    flattened_data = [turn for conversation in data for turn in conversation if isinstance(conversation, list)]
    return flattened_data

def extract_dialogues(data):
    processed_data = []
    def recursive_extraction(entry):
        if isinstance(entry, dict):
            processed_data.append(entry)
        elif isinstance(entry, list):
            for sub_entry in entry:
                recursive_extraction(sub_entry)
    recursive_extraction(data)
    return processed_data


# Use the
def format_prompts(examples):
    return {
        "text": [
            f"{role}: {text}" for role, text in zip(examples["role"], examples["text"])
        ]
    }

# Apply formatting
def reformat_and_split(input_file, output_dir, test_size=.2, seed=42):
    global dataset  # Declare that we're modifying the global variable
    flattened_data = flatten_data(input_file)
    processed_data = extract_dialogues(flattened_data)

    dataset = Dataset.from_dict({
    "role": [entry["role"] for entry in processed_data],
    "text": [entry["text"] for entry in processed_data]
    })
    dataset = dataset.map(format_prompts, batched=True)

    # Split the dataset into training and evaluation sets
    split_data = dataset.train_test_split(test_size=test_size, seed=seed)
    train_data = split_data["train"]
    eval_data = split_data["test"]

    # Save the split datasets
    train_output_path = os.path.join(output_dir, "train_data.json")
    eval_output_path = os.path.join(output_dir, "eval_data.json")

    train_data.to_json(train_output_path)
    eval_data.to_json(eval_output_path)

    print(f"Example from training data: {train_data['text'][0]}")
    print(f"Example from evaluation data: {eval_data['text'][0]}")

    return train_data, eval_data

# Check the formatted output
if __name__ == "__main__":
    input_file = "/content/drive/MyDrive/smart_home_chatbot/smart_home_dialogues_final.json"
    output_dir = "output_data"
    os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist
    train_data, eval_data = reformat_and_split(input_file, output_dir)

# Set up the model and tokenizer

# In[79]:


from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from peft import prepare_model_for_kbit_training
from huggingface_hub import login


token = "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"
login(token=token)

import torch
torch.cuda.empty_cache()

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model_id = "mistralai/Mistral-7B-Instruct-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.padding_side = "right"

from accelerate import init_empty_weights
from accelerate.utils import set_module_tensor_to_device

with init_empty_weights():
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",  # Automatically balance between CPU and GPU
        low_cpu_mem_usage=True,  # Reduces memory during loading
        token=token
    )


model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)


# Step 3: Set up PEFT (Parameter-Efficient Fine-Tuning)

# In[81]:


from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ],
    bias="none",
    lora_dropout=0.1,
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, config)


# Step 4: Set up the training arguments

# In[ ]:


from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="smart_home_chatbot",
    overwrite_output_dir=True,
    logging_dir="./logs",
    logging_steps=50,
    logging_first_step=True,            # Log the first step
    evaluation_strategy="steps",        # Evaluate during training at each `eval_steps`
    eval_steps=500,                      # Evaluation interval
    save_steps=500,                      # Save checkpoint interval
    save_total_limit=1,                 # Only keep the last 2 checkpoints
    disable_tqdm=False,                 # Show the progress bar
    num_train_epochs=3, # replace this, depending on your dataset
    per_device_eval_batch_size=4,       # Batch size for evaluation
    per_device_train_batch_size=4,
    gradient_accumulation_steps=32,
    learning_rate=1e-4,
    optim="adamw_8bit",
    fp16=True,
    report_to=["tensorboard"],
    dataloader_pin_memory=True,
    tf32=True
)


# Step 5: Initialize the trainer and fine-tune the model

# In[ ]:


from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

train_dataset = load_dataset("json", data_files="output_data/train_data.json")["train"]
eval_dataset = load_dataset("json", data_files="output_data/eval_data.json")["train"]


# Initialize the SFTTrainer
trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    max_seq_length=256,                # Pass directly
    dataset_text_field="text",         # Pass directly
    eval_dataset=eval_dataset
)

# Start training
trainer.train()

# Step 6: Merge the adapter and model back together

# In[ ]:


adapter_model = trainer.model
merged_model = adapter_model.merge_and_unload()

trained_tokenizer = trainer.tokenizer

